{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MASTER CDE: hands-on practical sessions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Playing with an hyperspectral image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now going to load and visualize an hyperspectral image. Urban is one of the most widely used hyperspectral data generally in the hyperspectral unmixing study. There are 307 x 307 pixels, each of which corresponds to a 2 x 2 $m^2$ area. In this image, there are 210 wavelengths ranging from 400 nm  to 2500 nm, resulting in a spectral resolution of 10 nm. After the channels 1--4, 76, 87, 101--111, 136--153 and 198--210 are removed (due to dense water vapor and atmospheric effects), giving a final image of 162  channels.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we need to load the dataset. It is available in the Matlab format under the name ```Urban_R162.mat```. First you can try to load this file and examine its (dictionnary) structure. If you are under Google collab, you need to find a way to load it from a google drive."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, using ```pl.imshow```, try to plot some bands and observe the spatial structure of the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "by associating band 10 to Red, 100 to Green and 150 to Blue, try to display the image as a false color image "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Final exercice: on a graphic, that shows wavelengths as X-axis, and the corresponding magnitude on the Y-axis, display 10 randomly chosen spectra (i.e. over 10 different pixels) on the same figure. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the mean spectrum present in the image. Display the variance along wavelength using the ```fill_between``` function from ```matplotlib```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pandas for Data Scientists"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(This part is adapted from the book \"Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications\"\n",
    "\n",
    "http://www.springer.com/gp/book/9783319500164)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we will learn the basis of the Pandas library for data scientists. \n",
    "\n",
    "Let us begin by importing those packages that we will need for our program. In the first cell we put the code to import the `pandas` library as `pd`. This is for convenience; every time we need to use some functionality from the pandas library, we will write `pd` instead of `pandas`. We will also import the two core libraries `numpy` library as `np` and `matplotlib.pyplot` library as `plt`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pylab as plt\r\n",
    "%matplotlib inline\r\n",
    "plt.style.use('seaborn-whitegrid')\r\n",
    "plt.rc('text', usetex=True)\r\n",
    "plt.rc('font', family='times')\r\n",
    "plt.rc('xtick', labelsize=10)\r\n",
    "plt.rc('ytick', labelsize=10)\r\n",
    "plt.rc('font', size=12)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DataFrames \n",
    "\n",
    "The key data structure in Pandas is the **DataFrame** object. A DataFrame is basically a tabular data structure, with rows and columns. Rows have a specific index to access them, which can be any name or value. In Pandas, the columns are called **Series**, a special type of data, which in essence consists of a list of several values, where each value has an index. \n",
    "Therefore, the DataFrame data structure can be seen as a spreadsheet, but it is much more flexible. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a new DataFrame from the scratch\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To understand how it works, let us see how to create a DataFrame from a common Python dictionary of lists.\n",
    "\n",
    "\n",
    "In this example, we use the pandas `DataFrame` object constructor with a dictionary of lists as argument.\n",
    "The value of each entry in the dictionary is the name of the column, and the lists are their values. \n",
    "The DataFrame columns can be arranged at construction time by entering a keyword \\emph{columns} with a list of the names of the columns ordered as we want. If the column keyword is not present in the constructor, the columns will be arranged in alphabetical order."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = {'year': [2010, 2011, 2012, 2010, 2011, 2012, 2010, 2011, 2012],\r\n",
    "        'team': ['FCBarcelona', 'FCBarcelona', 'FCBarcelona', 'RMadrid', 'RMadrid', 'RMadrid', 'ValenciaCF',\r\n",
    "                 'ValenciaCF', 'ValenciaCF'],\r\n",
    "        'wins':   [30, 28, 32, 29, 32, 26, 21, 17, 19],\r\n",
    "        'draws':  [6, 7, 4, 5, 4, 7, 8, 10, 8],\r\n",
    "        'losses': [2, 3, 2, 4, 2, 5, 9, 11, 11]\r\n",
    "        }\r\n",
    "football = pd.DataFrame(\r\n",
    "    data, columns=['year', 'team', 'wins', 'draws', 'losses'])\r\n",
    "football"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result is a table where each entry in the dictionary is a column. The index of each row is created automatically taking the position of its elements inside the entry lists, starting from 0.\n",
    "Although it is very easy to create DataFrames from scratch, most of the time what we will need to do is import chunks of data into a DataFrame structure, we will see how to do this in later examples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Open government data analysis example using Pandas\n",
    "\n",
    "To illustrate how we can use Pandas in a simple real problem, we will start doing some basic analysis of government data. For the sake of transparency, data produced by government entities must be open, meaning that they can be freely used, reused and distributed by anyone. An example of this is the Eurostat, which is the home of European Commission data. Eurostatâ€™s main role is to process and publish comparable statistical information at the European level. The data in Eurostat are provided by each member state and it is free to reuse them, both for noncommercial and commercial purposes (with some minor exceptions).  \n",
    "\n",
    "\n",
    "Since the amount of data in the Eurostat database is huge, in our first study we are only going to focus on data relative to indicators of educational funding by the member states. Thus, the first thing to do is to retrieve such data from Eurostat. Since open data have to be delivered in a plain text format, CSV (or any other delimiter-separated value) formats are commonly used to store tabular data. In a delimiter-separated value file, each line is a data record and each record consists of one or more fields, separated by the delimiter character (usually a comma). Therefore, the data we will use can be found already downloaded and preprocessed as a CSV file *educ_figdp_1_Data.csv*. Of course, it can also be downloaded as unprocessed tabular data from the Eurostat database site [Eurostat database site]( http://ec.europa.eu/eurostat/data/database) \n",
    "following the path:  `Tables by themes > Population and social conditions > \n",
    "Education and training > Education > Indicators on education finance > Public expenditure on education` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading tabular data\n",
    "\n",
    "Let us start reading the data we downloaded.\n",
    "The way to read CSV (or any other separated value, providing the separator character) files in Pandas is by calling the `read_csv` method. Besides the name of the file, we add the *na_values* key argument to this method along with the character that represents \"non available data\" in the file. Normally, CSV files have a header with the names of the columns. If this is the case,  we can use the *usecols* parameter to select which columns in the file will be used.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu = pd.read_csv('educ_figdp_1_Data.csv',\r\n",
    "                  na_values=':', usecols=['TIME', 'GEO', 'Value'])\r\n",
    "edu"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, the DataFrame resulting from reading our data is stored in **edu**. The output of the execution shows that the `edu` DataFrame size is 384 rows $\\times$ 3 columns. Since the DataFrame is too large to be fully displayed, three dots appear in the middle of each row.\n",
    "\n",
    "\n",
    "Beside this, Pandas also has functions for reading files with formats such as Excel, HDF5, tabulated files or even the content from the clipboard (`read_excel(), read\\_hdf(), read\\_table(), read\\_clipboard()`). Whichever function we use, the result of reading a file is stored as a DataFrame structure. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viewing Data\n",
    "\n",
    "To see how the data looks, we can use the `head()` method, which shows just the first five rows. if we put a number as an argument to this method, this will be the number of the first rows that are listed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, it exists the **tail()** method, which returns the last five rows by default."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.tail()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we want to know the names of the columns or the names of the indexes, we can use the DataFrame attributes **columns** and **index** respectively. The names of the columns or indexes can be changed by assigning a new list of the same length to these attributes. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.columns "
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.index"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values of any DataFrame can be retrieved as a Python array by calling its **values ** attribute."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.values"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we just want quick statistical information on all the numeric columns in a data frame, we can use the function **describe()**. The result shows the count, the mean, the standard deviation, the minimum and maximum, and the percentiles, by default, the 25th, 50th, and 75th, for all the values in each column or series.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.describe()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selection\n",
    "\n",
    "If we want to select a subset of data from a DataFrame, it is necessary to indicate this subset using square brackets **[]** after the DataFrame. The subset can be specified in several ways. If we want to select only one column from a DataFrame, we only need to put its name between the square brackets. The result will be a Series data structure, not a DataFrame, because only one column is retrieved.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu['Value']"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we want to select a subset of rows from a DataFrame, we can do so by indicating a range of rows separated by **:** inside the square brackets. This is commonly known as a *slice* of rows.\n",
    "\n",
    "Next instruction returns the slice of rows from the 10th to the 13th position. Note that the slice does not use the index labels as references, but the position. In this case, the labels of the rows simply coincide with the position of the rows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu[10:14]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another way to select a subset of data is by applying Boolean indexing. This indexing is commonly known as a *filter*. For instance, if we want to filter those values less than or equal to 6.5, we can do it like this: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu[edu['Value'] > 6.5].tail()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Boolean indexing uses the result of a Boolean operation over the data, returning a mask with True or False for each row. The rows marked True in the mask will be selected. In the previous example, the Boolean operation `edu['Value'] > 6.5` produces a Boolean mask. When an element in the *'Value'* column is greater than 6.5, the corresponding value in the mask is set to True, otherwise it is set to False. Then, when this mask is applied as an index in `edu[edu['Value'] > 6.5]`, the result is a filtered DataFrame containing only rows with values higher than 6.5. Of course, any of the usual Boolean operators can be used for filtering: < (less than), <= (less than or equal to), > (greater than), >= (greater than or equal to), =  (equal to), !=(not equal to)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering Missing Values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pandas uses the special value **NaN** (not a number) to represent missing values. In Python, `NaN` is a special floating-point value returned by certain operations when one of their results ends in an undefined value. A subtle feature of `NaN` values is that two `NaN` are never equal. Because of this, the only safe way to tell whether or not a value is missing in a DataFrame is by using the `isnull()` function. Indeed, this function can be used to filter rows with missing values:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu[edu['Value'].isnull()].head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Manipulating Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.max(axis=0)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we know how to select the desired data, the next thing we need to know is how to manipulate data. \n",
    "One of the most straightforward things we can do is to operate with columns or rows  using aggregation functions. The following list shows the most common aggregation functions.\n",
    "\n",
    "| Function  | Description | \n",
    "|-----------|-------------|\n",
    "| count()   |Number of non-null observations|  \n",
    "| sum()     |Sum of values|\n",
    "| mean()    |Mean of values            | \n",
    "| median()  |Arithmetic median of values             |\n",
    "| min()     |Minimum|\n",
    "| max()     |Maximum|\n",
    "| prod()    |Product of values|\n",
    "| std()     |Unbiased standard deviation|\n",
    "| var()     | Unbiased variance|\n",
    "\n",
    "The result of all these functions applied to a row or column is always a number. Meanwhile, if a function is applied to a DataFrame or a selection of rows and columns, then you can specify if the function should be applied to the rows for each column  (putting the **axis=0** keyword on the invocation of the function), or it should be applied on the columns for each row (putting the **axis=1** keyword on the invocation of the function)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Note that these are functions specific to Pandas, not the generic Python functions. There are differences in their implementation. In Python ,`NaN` values propagate through all operations without raising an exception. In contrast, Pandas operations exclude `NaN` values representing missing data. For example, the pandas **max** function excludes `NaN`  values, thus they are interpreted as missing values, while the standard Python **max** function will take the mathematical interpretation of `NaN` and return it as the maximum:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Pandas max function:', edu['Value'].max())\r\n",
    "print('Python max function:', max(edu['Value']))"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Beside these aggregation functions, we can apply operations over all the values in rows, columns or a selection of both. The rule of thumb is that an operation between columns means that it is applied to each row in that column and an operation between rows means that it is applied to each column in that row. For example we can apply any binary arithmetical operation (+,-,*,/) to an entire row:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s = edu['Value'] / 100\r\n",
    "s.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, we can apply any function to a DataFrame or Series just putting its name as argument  of the **apply** method. For example, in the following code, we apply the  **sqrt** function from the *numpy* library to perform the square root of each value in the *'Value'* column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s = edu['Value'].apply(np.sqrt)\r\n",
    "s.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we need to design a specific function to apply it, we can write an in-line function, commonly known as a $\\lambda$-function. A $\\lambda$-function is a function without a name. It is only necessary to specify the parameters it receives, between the **lambda**  keyword and the **:**. In the next example, only one parameter is needed, which will be the value of each element in the *'Value'* column. The value the function returns will be the square of that value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s = edu['Value'].apply(lambda d: d**2)\r\n",
    "s.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another basic manipulation operation is to set new values in our DataFrame. This can be done directly using the assign operator **=** over a DataFrame. For example, to add a new column to a DataFrame, we can assign a Series to a selection of a column that does not exist. This will produce a new column in the DataFrame after all the others. You must be aware that if a column with the same name already exists, the previous values will be overwritten. In the following example, we assign the Series that results from dividing the 'Value' column by the maximum value in the same column to a new column named 'ValueNorm'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu['ValueNorm'] = edu['Value'] / edu['Value'].max()\r\n",
    "edu.tail()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, if we  want to remove this column from the DataFrame, we can use the **drop** function; this removes the indicated rows if **axis=0**,  or the indicated columns if **axis=1**. In Pandas, all the functions that change the contents of a DataFrame, such as the drop function, will normally return a copy of the modified data, instead of overwriting the DataFrame. Therefore, the original DataFrame is kept. If you do not want to keep the old values, you can set the keyword **inplace** to `True`. By default, this keyword is set to `False`, meaning that a copy of the data is returned. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.drop('ValueNorm', axis=1, inplace=True)\r\n",
    "edu.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead, if what we want to do is to insert a new row at the bottom of the DataFrame, we can use the Pandas **append** function. This functions receives as argument the new row, which is represented as a dictionary where the keys are the name of the columns and the values the associated value. You must be aware to setting  the **ignore_index** flag in the **append** method  to `True`, otherwise the index 0 is given to this new row, what will produce an error if it already exists:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu = edu.append({'TIME': 2000, 'Value': 5.00, 'GEO': 'a'}, ignore_index=True)\n",
    "edu.tail()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, if we want to remove this row, we need to use the **drop** function again. Now we have to set the axis to 0, and specify the index of the row we want to remove. Since we want to remove the last row, we can use the max function over the indexes to determine which row is."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.drop(max(edu.index), axis=0, inplace=True)\n",
    "edu.tail()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To remove `NaN` values, instead of the generic drop function, we can use the specific **dropna()** function. If we want to erase any row that contains an `NaN` value, we have to set the **how** keyword to *any*. To restrict it to a subset of columns, we can specify it using the **subset** keyword. As we can see below, the result will we the same as using the **drop** function:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eduDrop = edu.dropna(how='any', subset=['Value'], axis=0)\n",
    "eduDrop.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If, instead of removing the rows containing `NaN`, we want to fill them with another value, then we can use the **fillna()** method, specifying which value has to be used. If we want to fill only some specific columns, we have to put as argument to the **fillna()** function a dictionary with the name of the columns as the key and which character to be used for filling as the value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eduFilled = edu.fillna(value={'Value': 0})\n",
    "eduFilled.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sorting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another important functionality we will need when inspecting our data is to sort by columns. We can sort a DataFrame using any column, using the **sort** function.  If we want to see the first five rows of data sorted in descending order  (i.e., from the largest to the smallest values) and using the *'Value'* column, then we just need to do this:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.sort_values(by='Value', ascending=False, inplace=True)\n",
    "edu.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the **inplace** keyword means that the DataFrame will be overwritten, and hence no new DataFrame is returned. If instead of **ascending = False** we use **ascending = True**, the values are sorted in ascending order (i.e. from the smallest to the largest values).\n",
    "\n",
    "If we want to return to the original order, we can sort by an index using the **sort_index** function and specifying **axis=0**:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "edu.sort_index(axis=0, ascending=True, inplace=True)\n",
    "edu.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grouping Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another very useful way to inspect data is to group it according to some criteria. For instance, in our example it would be nice to group all the data by country, regardless of the year. Pandas has the **groupby** function that allows us to do just that. The value returned by this function is a special grouped DataFrame. To have a proper DataFrame as a result, it is necessary to apply an aggregation function. Thus, this function will be applied to all the values in the same group.\n",
    "\n",
    "\n",
    "For example, in our case, if we want a DataFrame showing the mean of the values for each country over all the years, we can obtain it by grouping according to country and using the mean function as the aggregation method for each group. The result would be  a DataFrame with countries as indexes and the mean values as the column:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "group = edu[['GEO', 'Value']].groupby('GEO').mean()\n",
    "group.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rearranging Data\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Up until now, our indexes have been just a numeration of rows without much meaning.  We can transform the arrangement of our data, redistributing the indexes and columns for better manipulation of our data, which normally leads to better performance. We can rearrange our data using the **pivot_table** function. Here, we can specify which columns will be the new indexes, the new values and the new columns. \n",
    "\n",
    "For example, imagine that we want to transform our DataFrame to a spreadsheet-like structure with the country names as the index, while the columns will be the years starting from 2006 and the values will be the previous *'Value'* column. To do this, first we need to filter out the data and then pivot it in this way:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filtered_data = edu[edu['TIME'] > 2005]\n",
    "pivedu = pd.pivot_table(filtered_data, values='Value',\n",
    "                        index=['GEO'], columns=['TIME'])\n",
    "pivedu.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ranking Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another useful visualization feature is to rank data. For example, we would like to know how  each country is ranked by year. To see this, we will use the pandas **rank** function. But first, we need to clean up our previous pivoted table a bit so that it only has real countries with real data. To do this, first we drop the Euro area entries and shorten the Germany name entry, using the **rename** function and then we drop all the rows containing any NaN, using the **dropna** function.\n",
    "\n",
    "Now we can perform the ranking using the **rank** function. Note here that the parameter `ascending=False` makes the ranking go from the highest values to the lowest values. The Pandas rank function supports different tie-breaking methods, specified with the **method** parameter. In our case, we use the  **first** method, in which ranks are assigned in the order they appear in the array, avoiding gaps between ranking."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pivedu = pivedu.drop(['Euro area (13 countries)',\n",
    "                      'Euro area (15 countries)',\n",
    "                      'Euro area (17 countries)',\n",
    "                      'Euro area (18 countries)',\n",
    "                      'European Union (25 countries)',\n",
    "                      'European Union (27 countries)',\n",
    "                      'European Union (28 countries)'\n",
    "                      ], axis=0)\n",
    "pivedu = pivedu.rename(\n",
    "    index={'Germany (until 1990 former territory of the FRG)': 'Germany'})\n",
    "pivedu = pivedu.dropna()\n",
    "pivedu.rank(ascending=False, method='first').head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we want to make a global ranking taking into account all the years, we can sum up all the columns and rank the result. Then we can sort the resulting values to retrieve the top 5 countries for the last 6 years, in this way: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "totalSum = pivedu.sum(axis=1)\n",
    "totalSum.rank(ascending=False, method='dense').sort_values().head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " Pandas DataFrames and Series can be plotted using the **plot** function, which uses the library for graphics *Matplotlib*.  \n",
    " \n",
    "For example, if we want to plot the accumulated values for each country over the last 6 years, we can take the  Series obtained in the previous example and plot it directly by calling the **plot** function: \n",
    "\n",
    "\n",
    "Note that if we want the bars ordered from the highest to the lowest value, we need to sort the values in the Series first. The parameter  **kind** used in the **plot** function defines which kind of graphic will be used. In our case, a bar graph. The parameter **style** refers to the style properties of the graphic, in our case, the color of bars is set to **b** (blue). The alpha channel can be modified adding a keyword parameter **alpha**  with a percentage, producing a more translucent plot. Finally, using the **title** keyword the name of the graphic can be set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "totalSum = pivedu.sum(axis=1).sort_values(ascending=False)\n",
    "totalSum.plot(kind='bar', style='b', alpha=0.4,\n",
    "              title='Total Values for Country')\n",
    "plt.savefig('Totalvalue_Country.png', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is also possible to plot a DataFrame directly. In this case, each column is treated as a separated Series. For example, instead of printing the accumulated value over the years, we can plot the value for each year.\n",
    "\n",
    "\n",
    "In this case, we have used a horizontal bar diagram (`kind='barh'`) stacking all the years in the same country bar. This can be done by setting the parameter **stacked** to `True`. The number of default colors in a plot is only 5, thus if you have more than 5 Series to show, you need to specify more colors or otherwise the same set of colors will be used again. We can set a new set of colors using the keyword **color** with a list of colors. Basic colors have a single-character code assigned to each, for example, 'b' is for blue, 'r' for red, 'g' for green, 'y' for yellow, 'm' for magenta and 'c' for cyan. When several Series are shown in a plot, a legend is created for identifying each one. The name for each Series is the name of the column in the DataFrame. By default, the legend goes inside the plot area. If we want to change this, we can use the **legend** function of the axis object (this is the object returned when the plot function is called). By using the **loc** keyword, we can set the relative position of the legend with respect to the plot. It can be a combination of right or left and upper, lower or center. With**bbox_to_anchor** we can set an absolute position with respect to the plot, allowing us to put the legend outside the graph."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_colors = ['b', 'r', 'g', 'y', 'm', 'c']\n",
    "ax = pivedu.plot(kind='barh', stacked=True, color=my_colors, figsize=(12, 6))\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig('Value_Time_Country.png', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Further Reading\n",
    "Pandas has much more functionalities. Check out the (very readable) pandas docs if you want to learn more:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Practical exercice\n",
    "\n",
    "### Load iris from csv:\n",
    "We are going to work out of CSV file. Load the corresponding `iris.data.csv` as a pandas frame. The names of the variable are as following: ```names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# iris ="
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Numpy arrays from Pandas's dataframe:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# if pandas version >= 0.24 you can use .to_numpy()\n",
    "# sepal_widths = \n",
    "# petal_widths = \n",
    "# petal_lengths = \n",
    "# sepal_lengths = "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Descriptive Statistics:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "print ('The max of sepal length: ', sepal_lengths.max(), ' -- ', np.max(sepal_lengths))\n",
    "print ('The min of sepal length: ', sepal_lengths.min(),  ' -- ', np.min(sepal_lengths))\n",
    "print ('The mean of sepal length: ', sepal_lengths.mean(), ' -- ',  np.mean(sepal_lengths))\n",
    "print ('The STD of sepal length: ', sepal_lengths.std(),  ' -- ', np.std(sepal_lengths))\n",
    "print ('The variance of sepal length', sepal_lengths.var(),  ' -- ', np.var(sepal_lengths))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Median and Percentile:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy.stats import scoreatpercentile\n",
    "print ('The median of sepal length: ', np.median(sepal_lengths))\n",
    "print ('Percentile or quartile 50%: ', scoreatpercentile(sepal_lengths, 50))\n",
    "print ('Percentile or quartile 75%: ', scoreatpercentile(sepal_lengths, 75))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pandas statistics:\n",
    "\n",
    "`pandas` builts on top of `numpy` and provides more convenient approach.  \n",
    "\n",
    "### Dataframe's describe:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iris.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataframe's plot:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#ipython magic function\n",
    "%matplotlib inline\n",
    "iris.boxplot(return_type='axes')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iris.quantile([0.1, 0.9])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**exercice** Compute the empirical covariance matrix from the iris dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the eigen vectors from this covariance matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Project the data on first two eigen vectors (sorted along their eigen values. Visualize the corresponding 2D dataset using a ```scatterplot```from `matplotlib`. What is the corresponding statistical method associated to this process ?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "name": "Python basics",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}