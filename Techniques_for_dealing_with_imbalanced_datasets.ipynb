{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Techniques for dealing with imbalanced datasets",
      "provenance": [],
      "authorship_tag": "ABX9TyOA8OslmyubgH5pYVovn0PC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruphai/UBS/blob/main/Techniques_for_dealing_with_imbalanced_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample code for dealing with imbalanced datasets using the PyTorch API and the Sklearn Sampler."
      ],
      "metadata": {
        "id": "JCVeKbkGLuJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation\n",
        "\n",
        "The WiDS datasets is largely unbalanced, having more class labels with no palm plantations (0) than those with palm plantations. To bulwark for the possible challenges that this imbalanced sets could cause in the model training, we employ a commonly used technique in common vision for improve model performance known as Data Augmentation. <a href=\"https://vinesmsuic.github.io/2020/08/11/cnn-dataaug/#reference\">Data Augmentation </a> is a technique that is used to increase the diversity of data available for trainig models, without collecting new data. The most used data augmentation techniques include cropping, padding, horizontal and vertical flipping and other affine transformations. PyTorch Transforms implements most of these functions, and are usually employed with the Albumentations framework.\n",
        "When faced with the challenge of imbalanced datasets, we can employ one or a combination of the following strategies:\n",
        "- Undersampling, \n",
        "- Oversampling, \n",
        "- Class weighting, \n",
        "- Focal loss, \n",
        "\n",
        "Here, we will employ the oversampling technique. Oversampling is simply increasing the number of samples in the minor samples so as to reach a near equal or equal number of samples in the datasets. This approach is more suited for deep learning approaches since having more datasets could possibily increase the feature learning rather than using less data. \n",
        "\n",
        "https://medium.com/analytics-vidhya/handling-imbalanced-dataset-in-image-classification-dc6f1e13aeee\n"
      ],
      "metadata": {
        "id": "SoAvFaJYLZ-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m3AMMY4aLTX-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "\n",
        "# loss_fn = nn.CrossEntropyLoss(weights=torch.tensor([1, 8]))\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_DIR = os.path.join(project_path, \"train\")\n",
        "IMG_DIR"
      ],
      "metadata": {
        "id": "MJsep8ypLi8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_DIR = os.path.join(project_path, \"train\")\n",
        "# define transform function for normalizing the data.\n",
        "img_dir = '../assignment/train'\n",
        "\n",
        "def update_loader(IMG_DIR, dataframe, batch_size):\n",
        "    aug_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.RandomHorizontalFlip(p=0.25), \n",
        "        transforms.RandomVerticalFlip(p=0.25),                     \n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])# normalize data\n",
        "        ])\n",
        "    \n",
        "    dataset = ImageDataset(train_df, IMG_DIR, transform=aug_transforms)\n",
        "\n",
        "    class_weights = []\n",
        "    sample_weights = [0]*len(dataset)\n",
        "\n",
        "    for df_len in dataframe[\"has_oilpalm\"].value_counts():\n",
        "        class_weights.append( (len(dataframe) - df_len))\n",
        "\n",
        "    for idx, (data, label) in enumerate(dataset):\n",
        "        class_weight = class_weights[label]\n",
        "        sample_weights[idx] = class_weight\n",
        "\n",
        "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "    return loader\n",
        "\n",
        "def main():\n",
        "    loader = update_loader(IMG_DIR=IMG_DIR, dataframe=train_df, batch_size=16)\n",
        "\n",
        "    num_hasoilpalm = 0\n",
        "    num_nooilpalm = 0\n",
        "    \n",
        "    for epoch in range(2):\n",
        "        for data, labels in loader:\n",
        "            num_hasoilpalm += torch.sum(labels ==1)\n",
        "            num_nooilpalm += torch.sum(labels == 0)\n",
        "    \n",
        "    print(num_hasoilpalm)\n",
        "    print(num_nooilpalm)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "F74J8RerLk-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Oversampling as a sampling technique to balance our datasets.\n",
        "from sklearn.utils import resample\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "has_oilpalm_hp = train_df[(train_df['has_oilpalm'] == 1)].reset_index(drop=True)\n",
        "no_oilpalm_hp = train_df[(train_df['has_oilpalm'] == 0)].reset_index(drop=True)\n",
        "\n",
        "has_oilpalm_upsampled = resample(has_oilpalm_hp,\n",
        "                          replace=True,\n",
        "                          n_samples=len(no_oilpalm_hp),\n",
        "                          random_state=RANDOM_SEED)\n",
        "\n",
        "# balanced dataset\n",
        "balanced_data=pd.concat([has_oilpalm_upsampled, no_oilpalm_hp]).reset_index(drop=True)\n",
        "balanced_data[\"has_oilpalm\"].value_counts().plot(kind=\"bar\")"
      ],
      "metadata": {
        "id": "nTJ6ChxGLnOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(balanced_data)\n",
        "print(torch.tensor(1/np.bincount(train_df['has_oilpalm'])) )\n",
        "\n",
        "len(np.bincount(train_df['has_oilpalm']))"
      ],
      "metadata": {
        "id": "bPrzS_x5LrvC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}